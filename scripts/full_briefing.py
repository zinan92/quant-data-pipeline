#!/usr/bin/env python3
"""
å®Œæ•´å¸‚åœºç®€æŠ¥ v1 â€” 7å¤§æ¨¡å—ä¸€æ¬¡æ€§è¾“å‡º
=============================================
ç”¨æ³•: python scripts/full_briefing.py
è¾“å‡º: å®Œæ•´ç®€æŠ¥æ–‡æœ¬åˆ° stdoutï¼Œå¯ç›´æ¥æ¨é€

æ¨¡å—:
1. Aè‚¡æŒ‡æ•°        â€” æœ¬åœ°APIå®æ—¶è¡Œæƒ…
2. å¼‚åŠ¨ç»Ÿè®¡        â€” /api/news/market-alerts
3. ç›˜ä¸­å…¨ç¨‹å›é¡¾è¡¨æ ¼  â€” today_index_snapshots.json
4. FLOW-TOP20     â€” akshare å®æ—¶æ¦‚å¿µèµ„é‡‘æµ
5. ğŸ§  Wendyåˆ†æ   â€” è§„åˆ™å¼•æ“ï¼Œçº¯ç¡®å®šæ€§
6. è‡ªé€‰è‚¡å¼‚åŠ¨      â€” è‡ªé€‰è‚¡æ¶¨è·Œæ’è¡Œ
7. å¿«è®¯           â€” /api/news/latest

é™„åŠ : æ¯æ¬¡è¿è¡Œè‡ªåŠ¨ä¿å­˜æŒ‡æ•°å¿«ç…§
"""

import sys
import json
import time
import traceback
import requests
from datetime import datetime
from pathlib import Path

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = Path(__file__).parent.parent
SNAPSHOT_FILE = PROJECT_ROOT / "data" / "snapshots" / "intraday" / "today_index_snapshots.json"
SNAPSHOT_FILE.parent.mkdir(parents=True, exist_ok=True)

API_BASE = "http://127.0.0.1:8000"
SINA_HEADERS = {"Referer": "https://finance.sina.com.cn", "User-Agent": "Mozilla/5.0"}

INDEX_CODES = [
    ("000001.SH", "ä¸Šè¯æŒ‡æ•°"),
    ("399001.SZ", "æ·±è¯æˆæŒ‡"),
    ("399006.SZ", "åˆ›ä¸šæ¿æŒ‡"),
    ("000688.SH", "ç§‘åˆ›50"),
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Helper: safe section wrapper
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def safe_section(name):
    """Decorator: if a section fails, print error and continue."""
    def decorator(fn):
        def wrapper(*args, **kwargs):
            try:
                return fn(*args, **kwargs)
            except Exception as e:
                return [f"âš ï¸ [{name}] è·å–å¤±è´¥: {e}"]
        return wrapper
    return decorator


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 0. Save index snapshot (runs every time)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def save_index_snapshot(index_data: dict):
    """Save current index data as a snapshot point."""
    try:
        if SNAPSHOT_FILE.exists():
            snapshots = json.loads(SNAPSHOT_FILE.read_text())
            if snapshots.get("date") != datetime.now().strftime("%Y-%m-%d"):
                snapshots = {"date": datetime.now().strftime("%Y-%m-%d"), "snapshots": []}
        else:
            snapshots = {"date": datetime.now().strftime("%Y-%m-%d"), "snapshots": []}

        now_time = datetime.now().strftime("%H:%M")
        # Avoid duplicate timestamps
        existing_times = {s["time"] for s in snapshots["snapshots"]}
        if now_time in existing_times:
            return

        snapshot_entry = {"time": now_time, "indexes": {}}
        for code, info in index_data.items():
            snapshot_entry["indexes"][code] = {
                "name": info["name"],
                "price": info["price"],
                "pct": info["pct"],
            }

        snapshots["snapshots"].append(snapshot_entry)
        SNAPSHOT_FILE.write_text(json.dumps(snapshots, ensure_ascii=False, indent=2))
    except Exception:
        pass  # Non-critical


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Aè‚¡æŒ‡æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_indices() -> dict:
    """Fetch index data from local API."""
    result = {}
    for code, fallback_name in INDEX_CODES:
        try:
            r = requests.get(f"{API_BASE}/api/index/realtime/{code}", timeout=5)
            if r.ok:
                d = r.json()
                result[code] = {
                    "name": d.get("name", fallback_name),
                    "price": d.get("price", 0),
                    "pct": d.get("change_pct", 0),
                    "amount": d.get("amount", 0),
                    "last_update": d.get("last_update", ""),
                }
        except Exception:
            pass
    return result


@safe_section("Aè‚¡æŒ‡æ•°")
def section_indices(index_data: dict) -> list[str]:
    lines = ["ğŸ“ˆ **Aè‚¡æŒ‡æ•°**"]
    if not index_data:
        return lines + ["  æ•°æ®æš‚æ— "]

    for code, _ in INDEX_CODES:
        if code not in index_data:
            continue
        d = index_data[code]
        emoji = "ğŸŸ¢" if d["pct"] >= 0 else "ğŸ”´"
        sign = "+" if d["pct"] >= 0 else ""
        amt_yi = d["amount"] / 1e4 if d["amount"] else 0  # amount inä¸‡ â†’ äº¿
        lines.append(
            f"  {emoji} {d['name']}: {d['price']:.2f} ({sign}{d['pct']:.2f}%)"
            + (f" æˆäº¤:{amt_yi:.0f}äº¿" if amt_yi > 0 else "")
        )
    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. å¼‚åŠ¨ç»Ÿè®¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@safe_section("å¼‚åŠ¨ç»Ÿè®¡")
def section_alerts() -> list[str]:
    r = requests.get(f"{API_BASE}/api/news/market-alerts", timeout=10)
    data = r.json()

    limit_up = data.get("å°æ¶¨åœæ¿", {})
    limit_down = data.get("å°è·Œåœæ¿", {})
    big_buy = data.get("å¤§ç¬”ä¹°å…¥", {})
    big_sell = data.get("å¤§ç¬”å–å‡º", {})

    up_count = limit_up.get("count", 0)
    down_count = limit_down.get("count", 0)
    buy_count = big_buy.get("count", 0)
    sell_count = big_sell.get("count", 0)

    # Top names
    up_names = [t["name"] for t in limit_up.get("top", [])[:5]]
    down_names = [t["name"] for t in limit_down.get("top", [])[:5]]

    up_str = "ã€".join(up_names) if up_names else "â€”"
    down_str = "ã€".join(down_names) if down_names else "â€”"
    net = buy_count - sell_count

    lines = [
        "âš¡ **å¼‚åŠ¨ç»Ÿè®¡**",
        f"  ğŸŸ¢ æ¶¨åœ: {up_count}åª | {up_str}",
        f"  ğŸ”´ è·Œåœ: {down_count}åª | {down_str}",
        f"  ğŸ’° å¤§ç¬”ä¹°å…¥: {buy_count}åª | ğŸ”» å¤§ç¬”å–å‡º: {sell_count}åª"
        f"ï¼ˆå‡€{'ä¹°' if net >= 0 else 'å–'}å…¥{abs(net)}åªå·®é¢ï¼‰",
    ]
    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ç›˜ä¸­å…¨ç¨‹å›é¡¾è¡¨æ ¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@safe_section("ç›˜ä¸­å›é¡¾")
def section_intraday_table() -> list[str]:
    if not SNAPSHOT_FILE.exists():
        return ["ğŸ“‹ **ç›˜ä¸­å…¨ç¨‹å›é¡¾**", "  æš‚æ— å¿«ç…§æ•°æ®"]

    data = json.loads(SNAPSHOT_FILE.read_text())
    snapshots = data.get("snapshots", [])
    if not snapshots:
        return ["ğŸ“‹ **ç›˜ä¸­å…¨ç¨‹å›é¡¾**", "  æš‚æ— å¿«ç…§æ•°æ®"]

    # Build table header
    lines = [f"ğŸ“‹ **ç›˜ä¸­å…¨ç¨‹å›é¡¾** ({data.get('date', 'ä»Šæ—¥')})"]

    # Track highs/lows per index
    idx_tracker = {}  # code -> {high_price, high_time, low_price, low_time}

    # Table header
    lines.append(f"{'æ—¶é—´':>6} | {'ä¸Šè¯æŒ‡æ•°':>10} | {'æ·±è¯æˆæŒ‡':>11} | {'åˆ›ä¸šæ¿æŒ‡':>10}")
    lines.append(f"{'â”€'*6} | {'â”€'*10} | {'â”€'*11} | {'â”€'*10}")

    for snap in snapshots:
        t = snap["time"]
        indexes = snap.get("indexes", {})

        cols = [f"{t:>6}"]
        for code in ["000001.SH", "399001.SZ", "399006.SZ"]:
            idx = indexes.get(code, {})
            price = idx.get("price", 0)
            pct = idx.get("pct", 0)

            if price > 0:
                sign = "+" if pct >= 0 else ""
                col_str = f"{price:.2f}({sign}{pct:.2f}%)"

                # Track high/low
                if code not in idx_tracker:
                    idx_tracker[code] = {
                        "name": idx.get("name", code),
                        "high_price": price, "high_time": t, "high_pct": pct,
                        "low_price": price, "low_time": t, "low_pct": pct,
                    }
                else:
                    tr = idx_tracker[code]
                    if price > tr["high_price"]:
                        tr["high_price"] = price
                        tr["high_time"] = t
                        tr["high_pct"] = pct
                    if price < tr["low_price"]:
                        tr["low_price"] = price
                        tr["low_time"] = t
                        tr["low_pct"] = pct
            else:
                col_str = "â€”"

            # Pad to match header width
            if code == "399001.SZ":
                cols.append(f"{col_str:>11}")
            else:
                cols.append(f"{col_str:>10}")
        lines.append(" | ".join(cols))

    # High/Low summary
    if idx_tracker:
        lines.append("")
        lines.append("ğŸ“ **é«˜ä½ç‚¹:**")
        for code in ["000001.SH", "399001.SZ", "399006.SZ"]:
            if code in idx_tracker:
                tr = idx_tracker[code]
                lines.append(
                    f"  {tr['name']}: "
                    f"é«˜ç‚¹ {tr['high_price']:.2f}({tr['high_pct']:+.2f}%) @{tr['high_time']} | "
                    f"ä½ç‚¹ {tr['low_price']:.2f}({tr['low_pct']:+.2f}%) @{tr['low_time']}"
                )

    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. FLOW-TOP20 (å®æ—¶æ¦‚å¿µèµ„é‡‘æµ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_concept_flow():
    """Fetch realtime concept flow via akshare."""
    import akshare as ak
    df = ak.stock_fund_flow_concept(symbol="å³æ—¶")
    return df


@safe_section("FLOW-TOP20")
def section_flow_top20() -> tuple[list[str], object]:
    """Returns (lines, df) â€” df is used by section_analysis."""
    df = fetch_concept_flow()

    total = len(df)
    net_in = len(df[df["å‡€é¢"] > 0])
    net_out = len(df[df["å‡€é¢"] <= 0])

    # Sort by å‡€é¢ descending (should already be, but ensure)
    df_sorted = df.sort_values("å‡€é¢", ascending=False).reset_index(drop=True)

    lines = [
        f"ğŸ’° **FLOW-TOP20 (å®æ—¶æ¦‚å¿µèµ„é‡‘æµ)**",
        f"å…±{total}ä¸ªæ¦‚å¿µ | {net_in}ä¸ªå‡€æµå…¥ | {net_out}ä¸ªå‡€æµå‡º",
        "",
    ]

    # Top 20 inflow
    top20 = df_sorted.head(20)
    for i, (_, row) in enumerate(top20.iterrows(), 1):
        name = row["è¡Œä¸š"]
        net = row["å‡€é¢"]
        pct = row["è¡Œä¸š-æ¶¨è·Œå¹…"]
        lead = row["é¢†æ¶¨è‚¡"]
        lead_pct = row["é¢†æ¶¨è‚¡-æ¶¨è·Œå¹…"]
        count = row["å…¬å¸å®¶æ•°"]
        sign = "+" if net >= 0 else ""
        lines.append(
            f"  {i:>2}. {name} {sign}{net:.2f}äº¿ | {pct:+.2f}% | "
            f"{count}åª | é¢†æ¶¨:{lead}({lead_pct:+.2f}%)"
        )

    # Bottom 5 outflow
    bot5 = df_sorted.tail(5).iloc[::-1]  # Most negative last
    lines.append("")
    lines.append("  ğŸ“‰ **å‡€æµå‡ºå‰5:**")
    for _, row in bot5.iterrows():
        name = row["è¡Œä¸š"]
        net = row["å‡€é¢"]
        pct = row["è¡Œä¸š-æ¶¨è·Œå¹…"]
        lines.append(f"  â€¢ {name} {net:.2f}äº¿ | {pct:+.2f}%")

    return lines, df_sorted


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ğŸ§  Wendyåˆ†æ (Rule-based, ZERO AI)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def section_analysis(index_data: dict, flow_df, alert_data: dict = None) -> tuple[list[str], dict]:
    """Returns (lines, signal_data) â€” signal_data used by section_summary."""
    try:
        lines, signal_data = _section_analysis_inner(index_data, flow_df, alert_data)
        return lines, signal_data
    except Exception as e:
        return [f"âš ï¸ [Wendyåˆ†æ] è·å–å¤±è´¥: {e}"], {}


def _section_analysis_inner(index_data: dict, flow_df, alert_data: dict = None) -> tuple[list[str], dict]:
    lines = ["ğŸ§  **Wendyåˆ†æ**"]
    signal_data = {}  # Collect all signal data for summary

    # â”€â”€ 5a. å¸‚åœºå®šæ€§: ä¸Šè¯ vs åˆ›ä¸šæ¿å‰ªåˆ€å·® â”€â”€
    sh_pct = index_data.get("000001.SH", {}).get("pct", 0)
    cy_pct = index_data.get("399006.SZ", {}).get("pct", 0)
    scissor = sh_pct - cy_pct  # Positive = ä¸Šè¯å¼ºäºåˆ›ä¸šæ¿

    if scissor > 1.0:
        market_tone = "âš ï¸ Risk OFF (å¤§ç›˜è‚¡é¿é™©ï¼Œå°ç›˜æ‰¿å‹)"
    elif scissor < -1.0:
        market_tone = "ğŸš€ Risk ON (æˆé•¿è‚¡æ´»è·ƒï¼Œèµ„é‡‘åšå¤šå°ç›˜)"
    elif sh_pct > 0.5 and cy_pct > 0.5:
        market_tone = "ğŸŸ¢ æ™®æ¶¨è¡Œæƒ…"
    elif sh_pct < -0.5 and cy_pct < -0.5:
        market_tone = "ğŸ”´ æ™®è·Œè¡Œæƒ…"
    else:
        market_tone = "âš–ï¸ ä¸­æ€§éœ‡è¡"

    lines.append(f"")
    lines.append(f"**å¸‚åœºå®šæ€§:** {market_tone}")
    lines.append(f"  ä¸Šè¯ {sh_pct:+.2f}% vs åˆ›ä¸šæ¿ {cy_pct:+.2f}% â†’ å‰ªåˆ€å·® {scissor:+.2f}%")

    # â”€â”€ 5b. èµ„é‡‘è½®åŠ¨ â”€â”€
    lines.append("")
    lines.append("**èµ„é‡‘è½®åŠ¨:**")
    if flow_df is not None and len(flow_df) > 0:
        top3_in = flow_df.head(3)
        top3_out = flow_df.tail(3).iloc[::-1]

        in_names = " / ".join(
            [f"{r['è¡Œä¸š']}(+{r['å‡€é¢']:.1f}äº¿)" for _, r in top3_in.iterrows()]
        )
        out_names = " / ".join(
            [f"{r['è¡Œä¸š']}({r['å‡€é¢']:.1f}äº¿)" for _, r in top3_out.iterrows()]
        )
        lines.append(f"  ğŸ”º ä¸»åŠ›æµå…¥: {in_names}")
        lines.append(f"  ğŸ”» ä¸»åŠ›æµå‡º: {out_names}")
    else:
        lines.append("  æ•°æ®æš‚æ— ")

    # â”€â”€ 5c. å…³é”®ä¿¡å· â”€â”€
    lines.append("")
    lines.append("**å…³é”®ä¿¡å·:**")

    # Signal 1: å‰ªåˆ€å·®
    lines.append(f"  â€¢ ä¸Šè¯/åˆ›ä¸šæ¿å‰ªåˆ€å·®: {scissor:+.2f}%")

    # Signal 2: å‡€æµå…¥/æµå‡ºæ¯”
    if flow_df is not None and len(flow_df) > 0:
        n_in = len(flow_df[flow_df["å‡€é¢"] > 0])
        n_out = len(flow_df[flow_df["å‡€é¢"] <= 0])
        total_concepts = len(flow_df)
        pct_in = n_in / total_concepts * 100 if total_concepts > 0 else 0
        ratio_str = f"{n_in}:{n_out}"
        lines.append(f"  â€¢ å‡€æµå…¥/æµå‡ºæ¯”: {ratio_str} ({pct_in:.0f}%æ¦‚å¿µå‡€æµå…¥)")

        # Total net flow
        total_net = flow_df["å‡€é¢"].sum()
        lines.append(f"  â€¢ å…¨å¸‚åœºæ¦‚å¿µå‡€é¢åˆè®¡: {total_net:+.1f}äº¿")
    else:
        n_in, n_out, pct_in, total_net = 0, 0, 0, 0

    # Signal 3: æ¶¨åœ/è·Œåœæ¯”
    up_count = 0
    down_count = 0
    if alert_data:
        up_count = alert_data.get("å°æ¶¨åœæ¿", {}).get("count", 0)
        down_count = alert_data.get("å°è·Œåœæ¿", {}).get("count", 0)
    if up_count + down_count > 0:
        ud_ratio = up_count / down_count if down_count > 0 else float('inf')
        lines.append(f"  â€¢ æ¶¨åœ/è·Œåœæ¯”: {up_count}:{down_count} ({ud_ratio:.1f}x)")
    else:
        ud_ratio = 1.0

    # â”€â”€ 5d. ğŸ›¡ï¸ æŠ¤ç›˜æŒ‡æ ‡ (é“¶è¡Œ+ä¿é™©+è¯åˆ¸) â”€â”€
    lines.append("")
    lines.append("**ğŸ›¡ï¸ æŠ¤ç›˜æŒ‡æ ‡:**")
    # Map: display name â†’ search keyword (åŒèŠ±é¡ºæ¦‚å¿µå: å‚è‚¡é“¶è¡Œ/å‚è‚¡ä¿é™©/å‚è‚¡åˆ¸å•†)
    è­·ç›˜_sectors = {"é“¶è¡Œ": "å‚è‚¡é“¶è¡Œ", "ä¿é™©": "å‚è‚¡ä¿é™©", "è¯åˆ¸": "å‚è‚¡åˆ¸å•†"}
    è­·ç›˜_data = {}  # display_name -> {net, pct, name}
    è­·ç›˜_total = 0
    è­·ç›˜_count = 0
    if flow_df is not None and len(flow_df) > 0:
        for display_name, search_key in è­·ç›˜_sectors.items():
            match = flow_df[flow_df["è¡Œä¸š"] == search_key]
            if len(match) == 0:
                # Fallback: fuzzy match
                match = flow_df[flow_df["è¡Œä¸š"].str.contains(search_key, na=False)]
            if len(match) > 0:
                row = match.iloc[0]
                net = row["å‡€é¢"]
                pct = row["è¡Œä¸š-æ¶¨è·Œå¹…"]
                è­·ç›˜_data[display_name] = {"net": net, "pct": pct, "name": row["è¡Œä¸š"]}
                è­·ç›˜_total += net
                if net > 0:
                    è­·ç›˜_count += 1

        sector_strs = []
        for display_name in è­·ç›˜_sectors:
            v = è­·ç›˜_data.get(display_name)
            if v is not None:
                emoji = "ğŸŸ¢" if v["net"] > 0 else "ğŸ”´"
                sector_strs.append(f"{emoji}{display_name} {v['net']:+.1f}äº¿({v['pct']:+.2f}%)")
            else:
                sector_strs.append(f"âšª{display_name} æ— æ•°æ®")
        lines.append(f"  {' | '.join(sector_strs)}")

        if è­·ç›˜_count == 3:
            lines.append(f"  âš ï¸ ä¸‰å¤§é‡‘èæ¿å—å…¨éƒ¨å‡€æµå…¥({è­·ç›˜_total:+.1f}äº¿) â†’ **å›½å®¶æŠ¤ç›˜ä¿¡å·**ï¼Œç§‘æŠ€/æˆé•¿æŠ›å‹å¤§")
        elif è­·ç›˜_count >= 2:
            lines.append(f"  ğŸŸ¡ {è­·ç›˜_count}/3é‡‘èæ¿å—å‡€æµå…¥({è­·ç›˜_total:+.1f}äº¿) â†’ æœ‰æŠ¤ç›˜è¿¹è±¡")
        elif è­·ç›˜_total < -10:
            lines.append(f"  ğŸŸ¢ é‡‘èæ¿å—å‡€æµå‡º({è­·ç›˜_total:+.1f}äº¿) â†’ æ— éœ€æŠ¤ç›˜ï¼Œèµ„é‡‘åœ¨è¿›æ”»")
        else:
            lines.append(f"  âš–ï¸ é‡‘èæ¿å—ä¸­æ€§({è­·ç›˜_total:+.1f}äº¿)")
    else:
        lines.append("  æ•°æ®æš‚æ— ")

    # â”€â”€ 5e. ğŸ“ è¶‹åŠ¿å¼ºåº¦æ ‡å°º â”€â”€
    lines.append("")
    lines.append("**ğŸ“ è¶‹åŠ¿å¼ºåº¦:**")
    trend_strength = "æœªçŸ¥"
    top1_net = 0
    top1_name = "æœªçŸ¥"
    cluster_counts = {}
    # Exclude broad/index-level concepts â€” only real sector themes count
    BROAD_CONCEPTS = [
        "è¯é‡‘æŒè‚¡", "åŒèŠ±é¡ºæ¼‚äº®", "åŒèŠ±é¡ºä¸­ç‰¹ä¼°", "èèµ„èåˆ¸", "æ·±è‚¡é€š",
        "æ²ªè‚¡é€š", "è¶…çº§å“ç‰Œ", "å‚è‚¡é“¶è¡Œ", "å‚è‚¡ä¿é™©", "å‚è‚¡åˆ¸å•†",
    ]
    if flow_df is not None and len(flow_df) > 0:
        theme_df = flow_df[~flow_df["è¡Œä¸š"].apply(
            lambda x: any(b in x for b in BROAD_CONCEPTS)
        )].reset_index(drop=True)

        if len(theme_df) == 0:
            theme_df = flow_df  # Fallback

        top1 = theme_df.iloc[0]
        top1_net = abs(top1["å‡€é¢"])
        top1_name = top1["è¡Œä¸š"]

        if top1_net >= 200:
            trend_strength = "ğŸ”¥ å¼ºè¶‹åŠ¿"
            trend_desc = f"ä¸»çº¿æ˜ç¡®ï¼Œå¯ä»¥è·Ÿ"
        elif top1_net >= 100:
            trend_strength = "ğŸ“Š ä¸­ç­‰è¶‹åŠ¿"
            trend_desc = f"æœ‰æ–¹å‘ä½†åŠ›åº¦ä¸€èˆ¬"
        else:
            trend_strength = "ğŸ˜¶ å¼±è¶‹åŠ¿/æ— ä¸»çº¿"
            trend_desc = f"èµ„é‡‘åˆ†æ•£ï¼Œæ— æ˜ç¡®æ–¹å‘"

        lines.append(f"  #1 {top1_name}: {top1['å‡€é¢']:+.1f}äº¿ â†’ {trend_strength}ï¼ˆ{trend_desc}ï¼‰")

        # TOP10 concentration check (using theme_df, excludes broad indices)
        top10 = theme_df.head(10)
        top10_names = top10["è¡Œä¸š"].tolist()
        # Simple sector clustering: check if keywords repeat
        sector_keywords = {
            "å…‰ä¼/ç”µæ± ": ["å…‰ä¼", "ç”µæ± ", "TOPCON", "BC", "HJT", "é’™é’›çŸ¿", "ç¡…"],
            "AI/ç§‘æŠ€": ["äººå·¥æ™ºèƒ½", "AI", "ç®—åŠ›", "èŠ¯ç‰‡", "æ•°æ®ä¸­å¿ƒ", "æœºå™¨äºº"],
            "æ–°èƒ½æºè½¦": ["æ–°èƒ½æºè½¦", "é”‚ç”µ", "å……ç”µæ¡©", "æ±½è½¦"],
            "ç…¤ç‚­/èƒ½æº": ["ç…¤ç‚­", "çŸ³æ²¹", "å¤©ç„¶æ°”", "èƒ½æº"],
        }
        cluster_counts = {}
        for label, keywords in sector_keywords.items():
            count = sum(1 for name in top10_names if any(kw in name for kw in keywords))
            if count >= 2:
                cluster_counts[label] = count

        if cluster_counts:
            dominant = max(cluster_counts, key=cluster_counts.get)
            lines.append(f"  TOP10é›†ä¸­åº¦: {dominant}å {cluster_counts[dominant]}/10 â€” ä»Šæ—¥å”¯ä¸€ä¸»çº¿")
            if len(cluster_counts) > 1:
                others = [f"{k}({v})" for k, v in cluster_counts.items() if k != dominant]
                lines.append(f"  å…¶ä»–çº¿ç´¢: {', '.join(others)}")
        else:
            lines.append(f"  TOP10é›†ä¸­åº¦: åˆ†æ•£ï¼Œæ— æ˜æ˜¾ä¸»çº¿é›†ä¸­")
    else:
        lines.append("  æ•°æ®æš‚æ— ")

    # â”€â”€ 5f. ğŸ· ç™½é…’/æ¶ˆè´¹é¿é™©ä¿¡å· â”€â”€
    lines.append("")
    lines.append("**ğŸ· é¿é™©ä¿¡å·:**")
    baijiu_net = None
    if flow_df is not None and len(flow_df) > 0:
        baijiu_match = flow_df[flow_df["è¡Œä¸š"].str.contains("ç™½é…’", na=False)]
        if len(baijiu_match) > 0:
            bj = baijiu_match.iloc[0]
            baijiu_net = bj["å‡€é¢"]
            bj_pct = bj["è¡Œä¸š-æ¶¨è·Œå¹…"]
            emoji = "ğŸŸ¢" if baijiu_net > 0 else "ğŸ”´"
            lines.append(f"  ç™½é…’æ¿å—: {emoji} {baijiu_net:+.1f}äº¿ ({bj_pct:+.2f}%)")

            if baijiu_net > 10 and è­·ç›˜_count >= 2:
                lines.append(f"  ğŸš¨ ç™½é…’+é‡‘èåŒæ—¶æµå…¥ â†’ **æç«¯é¿é™©æ¨¡å¼**ï¼Œç§‘æŠ€æŠ›å‹æå¤§")
            elif baijiu_net > 10:
                lines.append(f"  âš ï¸ ç™½é…’èµ„é‡‘æµå…¥ â†’ é˜²å¾¡æ€§é…ç½®ï¼ŒRisk OFFä¿¡å·")
            elif baijiu_net < -10:
                lines.append(f"  ğŸŸ¢ ç™½é…’èµ„é‡‘æµå‡º â†’ éé¿é™©ï¼Œèµ„é‡‘åè¿›æ”»")
            else:
                lines.append(f"  âš–ï¸ ç™½é…’ä¸­æ€§")
        else:
            lines.append("  ç™½é…’æ¿å—: æ— æ•°æ®")
    else:
        lines.append("  æ•°æ®æš‚æ— ")

    # â”€â”€ 5g. æ“ä½œå»ºè®® (Template-based) â”€â”€
    lines.append("")
    lines.append("**æ“ä½œå»ºè®®:**")

    # Determine market regime and give template advice
    signals_bullish = 0
    signals_bearish = 0

    # Scoring
    if sh_pct > 0.3:
        signals_bullish += 1
    if sh_pct < -0.3:
        signals_bearish += 1
    if cy_pct > 0.3:
        signals_bullish += 1
    if cy_pct < -0.3:
        signals_bearish += 1
    if pct_in > 50:
        signals_bullish += 1
    if pct_in < 30:
        signals_bearish += 1
    if up_count > down_count * 1.5:
        signals_bullish += 1
    if down_count > up_count * 1.5:
        signals_bearish += 1
    if flow_df is not None and len(flow_df) > 0:
        if total_net > 0:
            signals_bullish += 1
        elif total_net < -50:
            signals_bearish += 1

    # Park's Three Signals integration
    if è­·ç›˜_count == 3:
        signals_bearish += 1  # Full æŠ¤ç›˜ = bearish for growth
    if top1_net < 100:
        signals_bearish += 1  # Weak trend = no conviction
    elif top1_net >= 200:
        signals_bullish += 1  # Strong trend
    if baijiu_net is not None and baijiu_net > 10 and è­·ç›˜_count >= 2:
        signals_bearish += 2  # Extreme risk-off

    if signals_bullish >= 4:
        advice = "âœ… å¤šå¤´å ä¼˜ï¼Œå¯ç§¯æå‚ä¸å¼ºåŠ¿æ¿å—ï¼Œå…³æ³¨èµ„é‡‘æµå…¥TOPæ¦‚å¿µ"
    elif signals_bearish >= 4:
        advice = "ğŸ›‘ ç©ºå¤´å ä¼˜ï¼Œå»ºè®®å‡ä»“è§‚æœ›æˆ–ä»…åšç¡®å®šæ€§æœºä¼šï¼Œæ§åˆ¶ä»“ä½"
    elif signals_bullish >= 3 and signals_bearish <= 1:
        advice = "ğŸŸ¢ åå¤šæ ¼å±€ï¼Œå¯é€‚å½“å‚ä¸é¢†æ¶¨æ¿å—ï¼Œæ³¨æ„åˆ†æ•£é£é™©"
    elif signals_bearish >= 3 and signals_bullish <= 1:
        advice = "ğŸŸ¡ åå¼±æ ¼å±€ï¼Œè½»ä»“æ“ä½œï¼Œå…³æ³¨é˜²å¾¡æ¿å—å’Œè¶…è·Œåå¼¹æœºä¼š"
    elif scissor > 1.5:
        advice = "âš ï¸ å¤§å°ç›˜åˆ†åŒ–ä¸¥é‡ï¼Œå…³æ³¨æƒé‡è‚¡æœºä¼šï¼Œå›é¿å°ç›˜é¢˜æ"
    elif scissor < -1.5:
        advice = "ğŸ”„ é¢˜ææ´»è·ƒä½†æƒé‡æ‹–ç´¯ï¼Œç²¾é€‰å¼ºåŠ¿æ¦‚å¿µï¼Œå¿«è¿›å¿«å‡º"
    else:
        advice = "âš–ï¸ éœ‡è¡æ ¼å±€ï¼Œä¿æŒä»“ä½çµæ´»ï¼Œå…³æ³¨ä¸»çº¿æ–¹å‘ç¡®è®¤"

    lines.append(f"  {advice}")
    lines.append(f"  (å¤šå¤´ä¿¡å·: {signals_bullish} | ç©ºå¤´ä¿¡å·: {signals_bearish})")

    # Collect signal data for summary
    signal_data = {
        "sh_pct": sh_pct,
        "cy_pct": cy_pct,
        "scissor": scissor,
        "market_tone": market_tone,
        "è­·ç›˜_count": è­·ç›˜_count,
        "è­·ç›˜_total": è­·ç›˜_total,
        "è­·ç›˜_data": è­·ç›˜_data,
        "top1_net": top1_net,
        "top1_name": top1_name if 'top1_name' in dir() else "æœªçŸ¥",
        "trend_strength": trend_strength,
        "baijiu_net": baijiu_net,
        "signals_bullish": signals_bullish,
        "signals_bearish": signals_bearish,
        "advice": advice,
        "flow_df": flow_df,
        "cluster_counts": cluster_counts,
    }

    return lines, signal_data


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. ğŸ“ ç›˜åæ€»ç»“ (Template-based narrative)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@safe_section("ç›˜åæ€»ç»“")
def section_summary(index_data: dict, signal_data: dict, alert_data: dict = None) -> list[str]:
    """Generate a narrative summary using signal data. Template-based, deterministic."""
    if not signal_data:
        return []

    sh_pct = signal_data.get("sh_pct", 0)
    cy_pct = signal_data.get("cy_pct", 0)
    scissor = signal_data.get("scissor", 0)
    è­·ç›˜_count = signal_data.get("è­·ç›˜_count", 0)
    è­·ç›˜_total = signal_data.get("è­·ç›˜_total", 0)
    top1_net = signal_data.get("top1_net", 0)
    top1_name = signal_data.get("top1_name", "")
    trend_strength = signal_data.get("trend_strength", "")
    baijiu_net = signal_data.get("baijiu_net", 0) or 0
    signals_bullish = signal_data.get("signals_bullish", 0)
    signals_bearish = signal_data.get("signals_bearish", 0)
    flow_df = signal_data.get("flow_df")
    cluster_counts = signal_data.get("cluster_counts", {})

    lines = ["â•â•â• ğŸ“ æ€»ç»“ â•â•â•", ""]

    # â”€â”€ Headline: surface vs reality â”€â”€
    sh_sign = "æ¶¨" if sh_pct >= 0 else "è·Œ"
    cy_sign = "æ¶¨" if cy_pct >= 0 else "è·Œ"

    # Determine the day's character
    if è­·ç›˜_count == 3 and top1_net < 100 and baijiu_net > 10:
        day_type = "extreme_risk_off"
    elif è­·ç›˜_count >= 2 and top1_net < 100:
        day_type = "risk_off"
    elif top1_net >= 200:
        day_type = "strong_trend"
    elif top1_net >= 100:
        day_type = "moderate_trend"
    elif signals_bullish >= 4:
        day_type = "bullish"
    elif signals_bearish >= 4:
        day_type = "bearish"
    else:
        day_type = "mixed"

    # â”€â”€ Headline â”€â”€
    if day_type == "extreme_risk_off":
        if sh_pct > 0:
            lines.append(
                f"ä»Šå¤©è¡¨é¢ä¸Šæ²ªæŒ‡{sh_sign}{abs(sh_pct):.2f}%çœ‹ç€ä¸é”™ï¼Œ"
                f"ä½†ä¸‰ä¸ªä¿¡å·å…¨éƒ¨æŒ‡å‘åŒä¸€ä¸ªç»“è®ºï¼š**è¿™æ˜¯ä¸€ä¸ªæç«¯é¿é™©æ—¥**ã€‚"
            )
        else:
            lines.append(f"ä»Šå¤©ä¸‰å¤§ä¿¡å·å…¨éƒ¨äº®çº¢ç¯ï¼š**æç«¯é¿é™©æ—¥**ã€‚")
    elif day_type == "risk_off":
        lines.append(
            f"æ²ªæŒ‡{sh_sign}{abs(sh_pct):.2f}%ï¼Œåˆ›ä¸šæ¿{cy_sign}{abs(cy_pct):.2f}%ã€‚"
            f"èµ„é‡‘åé˜²å¾¡ï¼ŒæŠ¤ç›˜è¿¹è±¡æ˜æ˜¾ã€‚"
        )
    elif day_type == "strong_trend":
        lines.append(
            f"ä»Šå¤©ä¸»çº¿æ˜ç¡®ï¼š**{top1_name}**é¢†è¡”ï¼Œä¸»åŠ›å‡€æµå…¥{top1_net:.0f}äº¿ï¼Œ"
            f"å¼ºè¶‹åŠ¿æ—¥ã€‚"
        )
    elif day_type == "moderate_trend":
        lines.append(
            f"æ²ªæŒ‡{sh_sign}{abs(sh_pct):.2f}%ï¼Œ"
            f"**{top1_name}**ä»¥{top1_net:.0f}äº¿é¢†æ¶¨ï¼Œè¶‹åŠ¿ä¸­ç­‰åå¼ºã€‚"
        )
    elif day_type == "bullish":
        lines.append(f"å¤šå¤´å ä¼˜ï¼Œæ²ªæŒ‡{sh_sign}{abs(sh_pct):.2f}%ï¼Œå¸‚åœºæƒ…ç»ªåæš–ã€‚")
    elif day_type == "bearish":
        lines.append(f"ç©ºå¤´å ä¼˜ï¼Œæ²ªæŒ‡{cy_sign}{abs(cy_pct):.2f}%ï¼Œå¸‚åœºæ‰¿å‹æ˜æ˜¾ã€‚")
    else:
        lines.append(
            f"æ²ªæŒ‡{sh_sign}{abs(sh_pct):.2f}%ï¼Œåˆ›ä¸šæ¿{cy_sign}{abs(cy_pct):.2f}%ï¼Œ"
            f"æ–¹å‘ä¸æ˜æœ—ã€‚"
        )

    lines.append("")

    # â”€â”€ Three signals summary â”€â”€
    # 1. æŠ¤ç›˜
    if è­·ç›˜_count == 3:
        lines.append(f"1. æŠ¤ç›˜ä¿¡å·å…¨äº®({è­·ç›˜_total:+.1f}äº¿) â†’ å›½å®¶åœ¨ä¿æŒ‡æ•°")
    elif è­·ç›˜_count >= 2:
        lines.append(f"1. æŠ¤ç›˜ä¿¡å·éƒ¨åˆ†äº®({è­·ç›˜_count}/3ï¼Œ{è­·ç›˜_total:+.1f}äº¿) â†’ æœ‰æŠ¤ç›˜è¿¹è±¡")
    else:
        lines.append(f"1. æŠ¤ç›˜ä¿¡å·æœªäº® â†’ æ— éœ€æƒé‡æ‰˜åº•")

    # 2. è¶‹åŠ¿å¼ºåº¦
    if top1_net >= 200:
        lines.append(f"2. è¶‹åŠ¿å¼ºåº¦{top1_net:.0f}äº¿({top1_name}) â†’ ä¸»çº¿æ˜ç¡®ï¼Œå¯è·Ÿ")
    elif top1_net >= 100:
        lines.append(f"2. è¶‹åŠ¿å¼ºåº¦{top1_net:.0f}äº¿({top1_name}) â†’ æœ‰æ–¹å‘ä½†åŠ›åº¦ä¸€èˆ¬")
    else:
        lines.append(f"2. è¶‹åŠ¿å¼ºåº¦ä»…{top1_net:.0f}äº¿ â†’ æ— çœŸæ­£ä¸»çº¿")

    # 3. é¿é™©
    if baijiu_net > 10 and è­·ç›˜_count >= 2:
        lines.append(f"3. ç™½é…’({baijiu_net:+.1f}äº¿)+é‡‘èè”åŠ¨ â†’ èµ„é‡‘å…¨é¢é˜²å¾¡")
    elif baijiu_net > 10:
        lines.append(f"3. ç™½é…’æ¿å—æµå…¥({baijiu_net:+.1f}äº¿) â†’ é˜²å¾¡æ€§é…ç½®")
    elif baijiu_net < -10:
        lines.append(f"3. ç™½é…’æ¿å—æµå‡º({baijiu_net:+.1f}äº¿) â†’ èµ„é‡‘åè¿›æ”»ï¼Œéé¿é™©")
    else:
        lines.append(f"3. ç™½é…’æ¿å—ä¸­æ€§({baijiu_net:+.1f}äº¿) â†’ æ— æ˜æ˜¾é¿é™©ä¿¡å·")

    lines.append("")

    # â”€â”€ Notable moves (from flow data) â”€â”€
    if flow_df is not None and len(flow_df) > 0:
        # Dominant theme
        if cluster_counts:
            dominant = max(cluster_counts, key=cluster_counts.get)
            if top1_net < 200:
                lines.append(
                    f"å”¯ä¸€äº®ç‚¹æ˜¯{dominant}æ¿å—(TOP10å {cluster_counts[dominant]}å¸­)ï¼Œ"
                    f"ä½†åŠ›åº¦è¿œä¸åŠå¼ºè¶‹åŠ¿æ—¥(200-300äº¿)ã€‚"
                )
            else:
                lines.append(f"ä»Šæ—¥ä¸»çº¿ï¼š{dominant}æ¿å—å¼ºåŠ¿é¢†æ¶¨ã€‚")

        # Major outflows
        BROAD_CONCEPTS = [
            "è¯é‡‘æŒè‚¡", "åŒèŠ±é¡ºæ¼‚äº®", "åŒèŠ±é¡ºä¸­ç‰¹ä¼°", "èèµ„èåˆ¸", "æ·±è‚¡é€š",
            "æ²ªè‚¡é€š", "è¶…çº§å“ç‰Œ", "å‚è‚¡é“¶è¡Œ", "å‚è‚¡ä¿é™©", "å‚è‚¡åˆ¸å•†",
        ]
        theme_out = flow_df[~flow_df["è¡Œä¸š"].apply(
            lambda x: any(b in x for b in BROAD_CONCEPTS)
        )]
        if len(theme_out) > 0:
            worst3 = theme_out.tail(3).iloc[::-1]
            total_outflow = sum(abs(r["å‡€é¢"]) for _, r in worst3.iterrows())
            names = "ã€".join(r["è¡Œä¸š"] for _, r in worst3.iterrows())
            if total_outflow > 500:
                lines.append(f"{names}é­é‡åˆè®¡è¶…{total_outflow:.0f}äº¿å‡€æµå‡ºï¼ŒæŠ›å‹æå¤§ã€‚")
            elif total_outflow > 200:
                lines.append(f"{names}å‡€æµå‡ºåˆè®¡{total_outflow:.0f}äº¿ï¼Œèµ„é‡‘æŒç»­æ’¤ç¦»ã€‚")

    lines.append("")

    # â”€â”€ æ˜æ—¥å…³æ³¨ â”€â”€
    lines.append("**æ˜æ—¥å…³æ³¨ï¼š**")
    focus_points = []

    # Based on trend strength
    if top1_net < 100:
        focus_points.append(f"{top1_name}èƒ½å¦ä»å¼±è¶‹åŠ¿å‡çº§ä¸ºä¸­ç­‰è¶‹åŠ¿(>100äº¿)")
    elif top1_net < 200:
        focus_points.append(f"{top1_name}èƒ½å¦çªç ´200äº¿ç¡®è®¤å¼ºä¸»çº¿")

    # Based on æŠ¤ç›˜
    if è­·ç›˜_count >= 2:
        focus_points.append("ç§‘æŠ€æ¿å—èƒ½å¦ä¼ç¨³æ­¢è¡€ã€æŠ¤ç›˜åŠ›åº¦æ˜¯å¦å‡å¼±")

    # Based on scissors
    if abs(scissor) > 1.5:
        focus_points.append("å¤§å°ç›˜å‰ªåˆ€å·®èƒ½å¦æ”¶çª„")

    # Based on market tone
    if signals_bearish >= 4:
        focus_points.append("ç©ºå¤´ä¿¡å·æ˜¯å¦ç¼“å’Œ")
    elif signals_bullish >= 4:
        focus_points.append("å¤šå¤´åŠ¨èƒ½æ˜¯å¦æŒç»­")

    if not focus_points:
        focus_points.append("ä¸»çº¿æ–¹å‘ç¡®è®¤ä¸èµ„é‡‘æµå‘å˜åŒ–")

    for fp in focus_points:
        lines.append(f"  â€¢ {fp}")

    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. è‡ªé€‰è‚¡å¼‚åŠ¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@safe_section("è‡ªé€‰è‚¡å¼‚åŠ¨")
def section_watchlist() -> list[str]:
    # Get watchlist
    r = requests.get(f"{API_BASE}/api/watchlist", timeout=10)
    if r.status_code != 200:
        return ["â­ **è‡ªé€‰è‚¡å¼‚åŠ¨**", "  è·å–è‡ªé€‰è‚¡åˆ—è¡¨å¤±è´¥"]
    watchlist = r.json()
    if not watchlist:
        return ["â­ **è‡ªé€‰è‚¡å¼‚åŠ¨**", "  è‡ªé€‰è‚¡åˆ—è¡¨ä¸ºç©º"]

    tickers = [w["ticker"] for w in watchlist]
    name_map = {w["ticker"]: w["name"] for w in watchlist}

    # Fetch prices via Sina
    results = []
    for i in range(0, len(tickers), 50):
        batch = tickers[i:i + 50]
        codes = ",".join([f"sh{t}" if t.startswith("6") else f"sz{t}" for t in batch])
        try:
            pr = requests.get(
                f"http://hq.sinajs.cn/list={codes}",
                headers=SINA_HEADERS, timeout=10,
            )
            for line in pr.text.strip().split("\n"):
                if "hq_str_" in line and '"' in line:
                    code_part = line.split("hq_str_")[1].split("=")[0]
                    ticker = code_part[2:]
                    data = line.split('"')[1].split(",")
                    if len(data) > 4 and data[3] and data[2]:
                        try:
                            cur = float(data[3])
                            prev = float(data[2])
                            if prev > 0:
                                pct = (cur - prev) / prev * 100
                                results.append((name_map.get(ticker, ticker), ticker, pct, cur))
                        except (ValueError, ZeroDivisionError):
                            pass
        except Exception:
            pass
        time.sleep(0.2)

    if not results:
        return ["â­ **è‡ªé€‰è‚¡å¼‚åŠ¨**", "  æ— æ³•è·å–è¡Œæƒ…æ•°æ®"]

    results.sort(key=lambda x: x[2], reverse=True)
    top5 = results[:5]
    bot5 = results[-5:]

    lines = ["â­ **è‡ªé€‰è‚¡å¼‚åŠ¨**"]
    lines.append("  ğŸ“ˆ **æ¶¨å¹…å‰5:**")
    for name, ticker, pct, price in top5:
        emoji = "ğŸŸ¢" if pct >= 0 else "ğŸ”´"
        lines.append(f"    {emoji} {name}({ticker}) {pct:+.2f}% ç°ä»·:{price:.2f}")
    lines.append("  ğŸ“‰ **è·Œå¹…å‰5:**")
    for name, ticker, pct, price in bot5:
        emoji = "ğŸŸ¢" if pct >= 0 else "ğŸ”´"
        lines.append(f"    {emoji} {name}({ticker}) {pct:+.2f}% ç°ä»·:{price:.2f}")

    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. å¿«è®¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@safe_section("å¿«è®¯")
def section_news() -> list[str]:
    r = requests.get(f"{API_BASE}/api/news/latest", timeout=10)
    data = r.json()
    news_list = data.get("news", [])

    lines = ["ğŸ“° **å¿«è®¯**"]
    if not news_list:
        lines.append("  æš‚æ— å¿«è®¯")
        return lines

    for item in news_list[:8]:
        title = item.get("title", "")[:80]
        t = item.get("time", "")
        # Extract just HH:MM from time string
        if t and len(t) >= 16:
            t = t[11:16]
        lines.append(f"  â€¢ [{t}] {title}")

    return lines


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main: Assemble all sections
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    now = datetime.now()
    time_label = now.strftime("%Y-%m-%d %H:%M")

    output_lines = [
        f"{'â•' * 50}",
        f"ğŸ“Š **å…¨å¸‚åœºç®€æŠ¥** ({time_label})",
        f"{'â•' * 50}",
    ]

    # â”€â”€ 1. Aè‚¡æŒ‡æ•° â”€â”€
    index_data = fetch_indices()
    output_lines.extend(section_indices(index_data))

    # Save snapshot (side effect)
    if index_data:
        save_index_snapshot(index_data)

    output_lines.append("")

    # â”€â”€ 2. å¼‚åŠ¨ç»Ÿè®¡ â”€â”€
    output_lines.extend(section_alerts())
    output_lines.append("")

    # Fetch alert data for analysis section
    alert_data = None
    try:
        r = requests.get(f"{API_BASE}/api/news/market-alerts", timeout=10)
        alert_data = r.json()
    except Exception:
        pass

    # â”€â”€ 3. ç›˜ä¸­å…¨ç¨‹å›é¡¾è¡¨æ ¼ â”€â”€
    output_lines.extend(section_intraday_table())
    output_lines.append("")

    # â”€â”€ 4. FLOW-TOP20 â”€â”€
    flow_result = section_flow_top20()
    flow_df = None
    if isinstance(flow_result, tuple):
        flow_lines, flow_df = flow_result
        output_lines.extend(flow_lines)
    else:
        # Error case â€” flow_result is just lines
        output_lines.extend(flow_result)
    output_lines.append("")

    # â”€â”€ 5. Wendyåˆ†æ â”€â”€
    analysis_result = section_analysis(index_data, flow_df, alert_data)
    signal_data = {}
    if isinstance(analysis_result, tuple):
        analysis_lines, signal_data = analysis_result
        output_lines.extend(analysis_lines)
    else:
        output_lines.extend(analysis_result)
    output_lines.append("")

    # â”€â”€ 6. è‡ªé€‰è‚¡å¼‚åŠ¨ â”€â”€
    output_lines.extend(section_watchlist())
    output_lines.append("")

    # â”€â”€ 7. å¿«è®¯ â”€â”€
    output_lines.extend(section_news())

    # â”€â”€ 8. ç›˜åæ€»ç»“ â”€â”€
    summary_lines = section_summary(index_data, signal_data, alert_data)
    if summary_lines:
        output_lines.append("")
        output_lines.extend(summary_lines)

    output_lines.append("")
    output_lines.append(f"{'â•' * 50}")
    output_lines.append(f"â± ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')} | æ•°æ®ä»…ä¾›å‚è€ƒ")

    print("\n".join(output_lines))


if __name__ == "__main__":
    main()
